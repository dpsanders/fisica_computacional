{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos iterativos para la álgebra lineal numérica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los notebooks anteriores, vimos que podemos resolver EDPs lineales estacionarios al reducirlos, a través de una discretización en el espacio, a problemas de álgebra lineal, y vimos que hay **métodos directos** para resolverlos, como el método de eliminación Gaussiana (o factorización $LU$). Los métodos de esta índole se llaman \"directos\", ya que proveen un algoritmo que resuelve directamente el problema, con un algoritmo que está garantizado que termine después de un número de operaciones computacionales que se puede calcular.\n",
    "\n",
    "[1 -- Opcional] Para una matriz de tamaño $N \\times N$, encuentra la *complejidad computacional* del algoritmo de eliminación Gaussiana, es decir, cómo escala el número total de operaciones requeridas con el tamaño $N$ del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, también vimos que el método de eliminación Gaussiana resulta a menudo ser muy poco eficiente, ya que la matriz en cuestión tiene muchos ceros. Decimos que la matriz es **sparse** (inglés), o dispersa, rala, escasa, etc. en español. \n",
    "\n",
    "Existen otra clase muy distinta de algoritmos en la álgebra lineal numérica, los **métodos iterativos**. Si pensamos en los problemas estacionarios como literalmente el estado estacionario de una ecuación de evolución (artificial), es natural pensar en resolverlos con métodos iterativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El método de relajación "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo iterativo más sencillo sirve para resolver ecuaciones lineales de tipo Poisson:\n",
    "\n",
    "$$\\nabla^2 V(\\mathbf{x}) = -\\rho(\\mathbf{x})$$.\n",
    "\n",
    "Vimos en el notebook anterior que al discretizar el operador $\\nabla^2$ en 2D en una red cuadrada (con $\\delta x = \\delta y$), nos lleva a un sistema de ecuaciones (una para cada punto $(i,j)$ de la malla) de la forma\n",
    "\n",
    "$$u_{i,j+1} + u_{i,j-1} + u_{i-1,j} + u_{i+1,j} - 4 u_{ij} = - \\delta^2 \\rho(x_{ij}).$$\n",
    "\n",
    "Como vimos en el Examen 1, podemos convertir esta ecuación en método iterativo al despejar $u_{ij}$ e introducir un tiempo artificial:\n",
    "\n",
    "$$u_{ij}^{(t+1)} = \\textstyle \\frac{1}{4} \\left[ u_{i,j+1}^{(t)} + u_{i,j-1}^{(t)} + u_{i-1,j}^{(t)} + u_{i+1,j}^{(t)} + \\delta^2 \\rho(x_{ij}) \\right]. \\quad (*)$$\n",
    "\n",
    "Eso se llama el **método de Jacobi**. Se llama un método de **relajación**, ya que la solución se relaja hacia el estado estacionario. Nótese que es necesario contar con dos matrices: una al paso $t$, y una al paso $t+1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] (i) Implementa el método de Jacobi y utilízalo para resolver los mismos problemas físicos que en el notebook 20. Nótese que la condición inicial es arbitraria, pero ¡las condiciones de frontera son cruciales!\n",
    "\n",
    "(ii) ¿Está mejor este método o el de eliminación gaussiana para este problema? ¿Por qué?\n",
    "\n",
    "(iii) ¿A cuál ecuación parcial diferencial de evolución corresponde la ecuación (*)?  De esta forma, ¿cómo podemos interpretar el método?\n",
    "\n",
    "(iv) Calcula el estado estacionario (después de una evolución larga) y guárdala. Repite la evolución (desde la misma condición inicial) para calcular cómo cambia la distancia (en una norma adecuada) desde el estado estacionario en el tiempo, y grafícala. ¿Qué comportamiento tiene?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de Jacobi converge lentamente, y no siempre converge. Otra forma de ver al método de Jacobi es el siguiente.\n",
    "\n",
    "Descompongamos la matriz $\\mathsf{M}$ de interés como \n",
    "\n",
    "$$ \\mathsf{M} =  \\mathsf{D} + \\mathsf{R},$$ \n",
    "\n",
    "donde $\\mathsf{D}$ es la parte diagonal y $\\mathsf{R}$ el resto. \n",
    "\n",
    "El resolver $\\mathsf{M} \\cdot \\mathbf{x} = \\mathbf{b}$ es, entonces, equivalente a resolver\n",
    "\n",
    "$$\\mathsf{D} \\cdot \\mathbf{x} = \\mathbf{b} - \\mathsf{R} \\cdot \\mathbf{x}.$$\n",
    "\n",
    "El punto es que ahora $\\mathsf{D}$, siendo diagonal, es fácil de invertir, y llegamos al método iterativo\n",
    "\n",
    "$$\\mathbf{x}^{(t+1)} = \\mathsf{D}^{-1} \\cdot (\\mathbf{b} - \\mathsf{R} \\cdot \\mathbf{x}^{(t)}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El método de Gauss--Seidel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una versión más eficiente del método Jacobi es el método de **Gauss--Seidel**. La diferencia radica en que los nuevos valores calculados se *sobreescriben en la misma matriz* (en lugar de almacenarse en una matriz diferente). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] Implementa el método de Gauss--Seidel, y compara la distancia desde el estado estacionario con el método de Jacobi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe otro método, llamado SOR (\"Successive Over-Relaxation\") que combina los dos métodos y puede ser (bastante) más eficiente. [¡Proyecto final!]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
